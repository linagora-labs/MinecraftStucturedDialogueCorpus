{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_relations = {'Comment':0, 'Contrast':1, 'Correction':2, 'Question-answer_pair':3, 'Acknowledgement':4,'Elaboration':5,\n",
    "                 'Clarification_question':6, 'Conditional':7, 'Continuation':8, 'Result':9, 'Explanation':10, 'Q-Elab':11,\n",
    "                 'Alternation':12, 'Narration':13, 'Confirmation_question':14, 'Sequence':15, 'Break':16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: Same pre-processing as in previous finetuning notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "filename = home + '/data/TRAIN+VAL_407_bert.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, input_format, position_ids_compute, tokenize\n",
    "from bert_format import undersample, format_time, flat_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(filename, map_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split out a certain portion of validation data \n",
    "train_data = data[40:]\n",
    "valid_data = data[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_train, labels_train, raw_train = input_format(train_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_valid, labels_valid, raw_valid = input_format(valid_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenizer and token ids\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "put = ['1','0']\n",
    "colors = ['r', 'b', 'g', 'o', 'y', 'p']\n",
    "listx = ['b', 'c', 'd', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n']\n",
    "listy = ['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "listz = ['a', 'e', 'i', 'o', 'u', 'p', 'q', 'r', 'x', 'y', 'z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_tokens = [''.join([s, t, i, j, k]) for s in put\n",
    "                for t in colors\n",
    "                for i in listx\n",
    "                for j in listy\n",
    "                for k in listz]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(coord_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train, attention_masks_train, token_type_ids_train = tokenize(input_text_train, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_valid, attention_masks_valid, token_type_ids_valid = tokenize(input_text_valid, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute position ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids_train = position_ids_compute(tokenizer, input_ids_train, raw_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids_valid = position_ids_compute(tokenizer, input_ids_valid, raw_valid, labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids_train = torch.tensor(position_ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids_valid = torch.tensor(position_ids_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersample <br>\n",
    "For Bertlinear we use the undersample function because..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_format import undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all cands\n",
    "print(len(labels_train))\n",
    "#unattached cands\n",
    "print(sum([1 for i in labels_train if i[3] == 0]))\n",
    "#attached cands\n",
    "print(sum([1 for i in labels_train if i[3] == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_attach_train = [l[3] for l in labels_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_attach_valid = [l[3] for l in labels_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = torch.tensor(labels_train)\n",
    "labels_valid = torch.tensor(labels_valid)\n",
    "labels_attach_train = torch.tensor(labels_attach_train)\n",
    "labels_attach_valid = torch.tensor(labels_attach_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB need to choose a number to keep\n",
    "#usually about 60% of total candidates\n",
    "labels_train, labels_attach_train, input_ids_train, attention_masks_train, token_type_ids_train, position_ids_train = undersample(103400, labels_train, labels_attach_train, input_ids_train, attention_masks_train, token_type_ids_train, position_ids_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gather metadata from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make meta data\n",
    "meta_data_train = []\n",
    "for i in range(len(labels_train)):\n",
    "  lbs = labels_train[i].tolist()\n",
    "  meta_data_train.append([lbs[2], lbs[2]-lbs[1]])\n",
    "\n",
    "meta_data_valid = []\n",
    "for i in range(len(labels_valid)):\n",
    "  lbs = labels_valid[i].tolist()\n",
    "  meta_data_valid.append([lbs[2], lbs[2]-lbs[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create metadata batches\n",
    "def get_batches(len_data, batch_size):\n",
    "    indices = [i for i in range(len_data)]\n",
    "    batches = []\n",
    "    for i in range(len_data // batch_size + bool(len_data) % batch_size):\n",
    "        batches.append(indices[i * batch_size:(i + 1) * batch_size])\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(len(meta_data_train), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = get_batches(len(meta_data_valid), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_batches = []\n",
    "for ba in train_batches:\n",
    "  meta_train_batches.append([meta_data_train[b] for b in ba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_valid_batches = []\n",
    "for ba in valid_batches:\n",
    "  meta_valid_batches.append([meta_data_valid[b] for b in ba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from transformers import AdamW, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train, token_type_ids_train, position_ids_train, labels_attach_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train.size(), attention_masks_train.size(), position_ids_train.size(), labels_attach_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_valid.size(), attention_masks_valid.size(), position_ids_valid.size(), labels_attach_valid.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = TensorDataset(input_ids_valid, attention_masks_valid, token_type_ids_valid, position_ids_valid, labels_attach_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = SequentialSampler(train_dataset),\n",
    "            batch_size = 32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = 32\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load finetuned from first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = home + '<name of your model folder>/<name of your finetune .pth file output>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = True, attention_probs_dropout_prob=0, hidden_dropout_prob=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!resize embedder to account for new embeddings!\n",
    "embedder.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "embedder.load_state_dict(checkpoint['model_state_dict'])\n",
    "embedder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix neural net here\n",
    "#hidden_size = 774\n",
    "#hidden_size = 772 for just incoming rels\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            # nn.Linear(params.hidden_size, params.hidden_size_1),\n",
    "            nn.Linear(770, 2000),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2000, 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.train()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.AdamW(params=linear.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model_path = home + '<name of your model folder>'\n",
    "save_linear_name =  '<name of your bertlinear .pth file output>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(15):\n",
    "    loss_sum_train = 0\n",
    "    linear.train()\n",
    "    for e, batch in enumerate(train_dataloader):\n",
    "      if e in [0, len(train_dataloader)-1]:\n",
    "        print(\"epoch \", epoch)\n",
    "        print(\"batch no \", e)\n",
    "      output = embedder(batch[0].to(device),\n",
    "                        token_type_ids = batch[2].to(device),\n",
    "                        attention_mask = batch[1].to(device),\n",
    "                        position_ids = batch[3].to(device),\n",
    "                        labels = batch[4].to(device),\n",
    "                        return_dict=True)\n",
    "      #concat each candidate embedding with metadata tensor\n",
    "      #stack these\n",
    "      H_embed = torch.stack([torch.cat((r[0], torch.tensor(meta_train_batches[e][i]).to(device)),0) for i, r in enumerate(output.hidden_states[-1])])\n",
    "      H_embed = H_embed.to(device)\n",
    "      logits = linear(H_embed).unsqueeze(0)\n",
    "      logits = logits.squeeze(-1)\n",
    "\n",
    "   \n",
    "      target = torch.tensor([[float(b) for b in batch[4]]]).to(device)\n",
    "     \n",
    "\n",
    "      loss = criterion(input=logits, target=target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "      loss_sum_train += loss.item()\n",
    "\n",
    "    # Calculate the average train loss over all of the batches.\n",
    "    avg_train_loss = loss_sum_train / len(train_dataloader)\n",
    "    print(\"avg train loss: \", avg_train_loss)\n",
    "\n",
    "\n",
    "    print(\"going to eval\")\n",
    "    linear.eval()\n",
    "    loss_sum_valid = 0\n",
    "\n",
    "    for e, batch in enumerate(validation_dataloader):\n",
    "      with torch.no_grad():\n",
    "        output = embedder(batch[0].to(device),\n",
    "                        token_type_ids = batch[2].to(device),\n",
    "                        attention_mask = batch[1].to(device),\n",
    "                        position_ids = batch[3].to(device),\n",
    "                        labels = batch[4].to(device),\n",
    "                        return_dict=True)\n",
    "\n",
    "      H_embed = torch.stack([torch.cat((r[0], torch.tensor(meta_valid_batches[e][i]).to(device)),0) for i, r in enumerate(output.hidden_states[-1])])\n",
    "      H_embed = H_embed.to(device)\n",
    "      with torch.no_grad():\n",
    "            logits = linear(H_embed).unsqueeze(0)\n",
    "\n",
    "      logits = logits.squeeze(-1)\n",
    "\n",
    "      target = torch.tensor([[float(b) for b in batch[4]]]).to(device)\n",
    "      # target = batch[4].to(device)\n",
    "\n",
    "      loss = criterion(input=logits, target=target)\n",
    "\n",
    "      loss_sum_valid += loss.item()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = loss_sum_valid / len(validation_dataloader)\n",
    "    print(\"avg val loss: \", avg_val_loss)\n",
    "\n",
    "    print('--------------------------------------')\n",
    "\n",
    "output_model = linear_model_path + save_linear_name\n",
    "\n",
    "print('finished_training, saving to : ', output_model)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': linear.state_dict(),\n",
    "}, output_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get scores on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home=%pwd\n",
    "filename = home + '/data/TEST_101_bert.json'\n",
    "test_data = load_data(filename, map_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_test, labels_test, raw_test = input_format(test_data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test, attention_masks_test, token_type_ids_test = tokenize(input_text_test, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids_test = position_ids_compute(tokenizer, input_ids_test, raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids_test = torch.tensor(position_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_attach_test = [l[3] for l in labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_test = []\n",
    "for i in range(len(labels_test)):\n",
    "  lbs = labels_test[i]\n",
    "  meta_data_test.append([lbs[2], lbs[2]-lbs[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = get_batches(len(meta_data_test), 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_batches = []\n",
    "for ba in test_batches:\n",
    "  meta_test_batches.append([meta_data_test[b] for b in ba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_batches = []\n",
    "for ba in test_batches:\n",
    "  labels_test_batches.append([labels_test[b] for b in ba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test, token_type_ids_test, position_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset),\n",
    "            batch_size = 32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem is somewhere aroudn i = 2010\n",
    "for i, e in enumerate(test_dataloader):\n",
    "    if i == 2010:\n",
    "        print(e)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start :: if need to reload the linear model to run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = home + '<name of your model folder>/<name of your linear .pth file output>'\n",
    "linear = NeuralNetwork().to(device)\n",
    "checkpoint = torch.load(model_path, map_location='cuda')\n",
    "linear.load_state_dict(checkpoint['model_state_dict'])\n",
    "linear.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End :: if you needed to reload the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "linear.eval()\n",
    "\n",
    "for e, batch in enumerate(test_dataloader):\n",
    "  print(e)\n",
    "  with torch.no_grad():\n",
    "    output = embedder(batch[0].to(device),\n",
    "                    token_type_ids = batch[2].to(device),\n",
    "                    attention_mask = batch[1].to(device),\n",
    "                    position_ids = batch[3].to(device),\n",
    "                    # labels = batch[4].to(device),\n",
    "                    return_dict=True)\n",
    "\n",
    "  H_embed = torch.stack([torch.cat((r[0], torch.tensor(meta_test_batches[e][i]).to(device)),0) for i, r in enumerate(output.hidden_states[-1])])\n",
    "  H_embed = H_embed.to(device)\n",
    "  with torch.no_grad():\n",
    "        logits = linear(H_embed).unsqueeze(0)\n",
    "\n",
    "  m = nn.Sigmoid()\n",
    "  mod =(m(logits)).squeeze(-1).cpu().tolist()[0]\n",
    "  xs = [i for i in range(len(mod)) if mod[i] > 0.81]  \n",
    "  \n",
    "\n",
    "  labels = labels_test_batches[e]\n",
    "  for lab in range(len(labels)):\n",
    "    if lab in xs:\n",
    "      labels[lab].append(1)\n",
    "    else:\n",
    "      labels[lab].append(0)\n",
    "\n",
    "  predictions.extend(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_predictions = [i[5] for i in predictions]\n",
    "true_labels = [i[3] for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_recall_fscore_support(true_labels, attach_predictions, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change output to a list of lists so it can be fed to multitask <br>\n",
    "needs to be a list of lists, each list a game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multitask_inputs = []\n",
    "for i in range(101): #32 for the minecraft data\n",
    "    inputs = [[e[1], e[2]] for e in predictions if e[0] == i and e[5]==1]\n",
    "    multitask_inputs.append(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multitask_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(home + '<name of your pickle folder>/<name of your linear preds pickle file>', 'wb') as f:\n",
    "    pickle.dump(multitask_inputs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
